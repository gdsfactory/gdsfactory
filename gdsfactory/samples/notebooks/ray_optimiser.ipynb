{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d03883",
   "metadata": {},
   "source": [
    "# Ray Tune generic black-box optimiser\n",
    "\n",
    "[Ray Tune](https://docs.ray.io/en/latest/tune/index.html) is a hyperparameter tuning library primarily developed for machine learning.\n",
    "However, it is suitable for generic black-box optimisation as well.\n",
    "For our purpose, it provides an interface for running simulations inside a given *search space* and optimising for a given *loss function* $\\mathcal{L}$ using a given *algorithm*.\n",
    "It automatically manages checkpointing, logging and, importantly, parallel (or even distributed) computing.\n",
    "\n",
    "You can see installation instructions [here](https://docs.ray.io/en/latest/ray-overview/installation.html), but the regular pip install should work for most. Notably, ARM-based macOS support is experimental.\n",
    "\n",
    "```console\n",
    "pip install \"ray[tune,air]\" hyperopt\n",
    "```\n",
    "\n",
    "You can optimise a `mmi1x2` component for a transmission of $|S_{21}|^2 = 0.5$ (50% power) for a given wavelength using MEEP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93276a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import subprocess\n",
    "import importlib\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "import ray.air\n",
    "import ray.air.session\n",
    "from ray import tune\n",
    "\n",
    "import gdsfactory as gf\n",
    "import gdsfactory.simulation.gmeep as gm\n",
    "from gdsfactory.config import PATH\n",
    "from gdsfactory.generic_tech import get_generic_pdk\n",
    "\n",
    "gf.config.rich_output()\n",
    "PDK = get_generic_pdk()\n",
    "PDK.activate()\n",
    "\n",
    "tmp = PATH.optimiser\n",
    "tmp.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e708f50",
   "metadata": {},
   "source": [
    "## Loss function $\\mathcal{L}$\n",
    "\n",
    "The loss function is very important and should be designed to be meaningful for your need.\n",
    "\n",
    "The easiest method to optimise for a specific value is to use $L_1$ or $L_2$ (MSE) loss. Different optimisation algorithms might prefer more or less aggressive behaviour close to target, so choose depending on that.\n",
    "$$\n",
    "\\begin{align*}\n",
    "    L_1(x) &= |x_\\text{target} - x|, \\\\\n",
    "    L_2(x) &= \\left(x_\\text{target} - x\\right)^2\n",
    "    .\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a979769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_S21_L1(x, target):\n",
    "    r\"\"\"Loss function. Returns :math:`$\\sum_i L_1(x_i)$` and :math:`$x$` as a tuple\"\"\"\n",
    "    return np.abs(target - x), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce6588",
   "metadata": {},
   "source": [
    "Let's select a target of $0.7$ for $S_{21}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ea028",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = partial(loss_S21_L1, target=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c89f5",
   "metadata": {},
   "source": [
    "## Optimisation settings\n",
    "\n",
    "Here we specify the search space, the optimiser and its settings.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note</b> Choosing a new optimiser often requires you to install a separate package, see <a href=\"https://docs.ray.io/en/latest/tune/api_docs/suggestion.html\">Ray Tune → Search Algorithms</a> for details. Here one needs to install <a href=\"http://hyperopt.github.io/hyperopt/\">Hyperopt</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5284b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_config = {\n",
    "    \"length_mmi\": tune.uniform(0.05, 2),\n",
    "    \"width_mmi\": tune.uniform(0.05, 2),\n",
    "}\n",
    "\n",
    "# pylint: disable=wrong-import-position,ungrouped-imports\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "tune_config = tune.TuneConfig(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    search_alg=HyperOptSearch(),\n",
    "    max_concurrent_trials=2,  # simulations to run in parallel\n",
    "    num_samples=-1,  # max iterations, can be -1 for infinite\n",
    "    time_budget_s=60\n",
    "    * 20,  # time after which optimisation is stopped. May be useful along with ``num_samples=-1``.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f53e1f8",
   "metadata": {},
   "source": [
    "## Implement a *trainable* function\n",
    "\n",
    "You need to implement a function which can be *trained* to be improved w.r.t. our $\\mathcal{L}$.\n",
    "In other words, we create a function for a single training step, which generates, runs, and returns output $\\mathcal{L}(\\vec{x})$ from simulations for given parameters $\\vec{x}$. This may require a bit more effort and some shell scripting to get right depending on your simulations.\n",
    "\n",
    "Here we demonstrate a trainable for S-parameter simulations. The `write_sparameters_meep` returns $\\mathbf{S}$ as a function of $\\lambda$ given in $\\text{µm}$. From this, we select $S_{21}(\\lambda)$ and try to optimise for $\\min_\\text{geometry} \\sum_\\lambda (S_{21}(\\lambda) - \\text{target})$. In other words, that the transmission from 1 to 2 would be as close to target as possible for the given wavelength (or range of wavelengths).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable_simulations(config):\n",
    "    \"\"\"Training step, or `trainable`, function for Ray Tune to run simulations and return results.\"\"\"\n",
    "\n",
    "    # Component to optimise\n",
    "    component = gf.components.mmi1x2(**config)\n",
    "\n",
    "    # Simulate and get output\n",
    "    dirpath = tmp / ray.air.session.get_trial_id()\n",
    "\n",
    "    meep_params = dict(\n",
    "        component=component,\n",
    "        run=True,\n",
    "        dirpath=dirpath,\n",
    "        wavelength_start=1.5,\n",
    "        # wavelength_stop=1.6,\n",
    "        wavelength_points=1,\n",
    "    )\n",
    "\n",
    "    if use_mpi := True:  # change this to false if no MPI support\n",
    "        s_params = gm.write_sparameters_meep_mpi(\n",
    "            cores=2, **meep_params  # set this to be same as in `tune.Tuner`\n",
    "        )\n",
    "        s_params = np.load(s_params)  # parallel version returns filepath to npz instead\n",
    "    else:\n",
    "        s_params = gm.write_sparameters_meep(**meep_params)\n",
    "\n",
    "    s_params_abs = np.abs(s_params[\"o2@0,o1@0\"]) ** 2\n",
    "\n",
    "    loss_x, x = loss(s_params_abs)\n",
    "    if not np.isscalar(x):  # for many wavelengths, consider sum and mean\n",
    "        loss_x, x = loss_x.sum(), x.mean()\n",
    "\n",
    "    return {\"loss\": loss_x, \"value\": x}\n",
    "\n",
    "    # ALTERNATIVE\n",
    "    # For a shell-based solution to more software, subprocess.run is recommended roughly as below\n",
    "    # interpreter = shutil.which('bash')\n",
    "    # subprocess.run(\n",
    "    #     [interpreter, '-c', './generated_simulation.sh'],\n",
    "    #     cwd=dirpath,\n",
    "    #     check=True,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e73497",
   "metadata": {},
   "source": [
    "## Run optimiser\n",
    "In the cell below, we gather all the code above to a [`tune.Tuner`](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tuner) instance and start the optimisation by calling `tuner.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbd9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        trainable_simulations, {\"cpu\": 2}\n",
    "    ),  # maximum resources given to a worker, it also supports 'gpu'\n",
    "    param_space=search_config,\n",
    "    tune_config=tune_config,\n",
    "    run_config=ray.air.RunConfig(\n",
    "        local_dir=tmp / \"ray_results\",\n",
    "        checkpoint_config=ray.air.CheckpointConfig(checkpoint_frequency=1),\n",
    "        log_to_file=True,\n",
    "        verbose=2,  # Intermediate results in Jupyter\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Previous simulations can be restored with, see https://docs.ray.io/en/latest/tune/tutorials/tune-stopping.html\n",
    "# tuner = Tuner.restore(path=tmp / \"ray_results/my_experiment\")\n",
    "\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95cfa1f",
   "metadata": {},
   "source": [
    "The results can be seen and manipulated in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa7b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results.get_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f3e9c5",
   "metadata": {},
   "source": [
    "There are clearly many possible solutions, so making a [Pareto front](https://en.wikipedia.org/wiki/Pareto_front) plot w.r.t. some other parameter like overall size would make sense here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = results.get_best_result(metric=\"loss\", mode=\"min\").metrics\n",
    "best_params[\"loss\"], best_params[\"config\"]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
